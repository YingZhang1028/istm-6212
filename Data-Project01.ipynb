{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Word counts (40 points)\n",
    "### Part A. Characters in Little Women\n",
    "#### How many times are each of the following characters mentioned by name in the text of Little Women?\n",
    "#### • Jo, Beth, Meg, Amy\n",
    "#### Use the text available at https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-andschedule/master/projects/project-01/women.txt for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-16 09:57:14--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1053440 (1.0M) [text/plain]\n",
      "Saving to: ‘women.txt’\n",
      "\n",
      "women.txt           100%[===================>]   1.00M  3.36MB/s    in 0.3s    \n",
      "\n",
      "2016-09-16 09:57:15 (3.36 MB/s) - ‘women.txt’ saved [1053440/1053440]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355 Jo\r\n",
      " 683 Meg\r\n",
      " 645 Amy\r\n",
      " 459 Beth\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{2,}}' women.txt | grep -w \"Jo\\|Beth\\|Meg\\|Amy\" | sort | uniq -c |sort -rn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B. Juliet and Romeo in Romeo and Juliet\n",
    "#### How many times do each of the characters Juliet and Romeo have speaking lines in Romeo and Juliet? Keep in mind that this is the text of a play.\n",
    "#### Use the text available at https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-andschedule/master/projects/project-01/romeo.txt for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-15 10:50:09--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 178983 (175K) [text/plain]\n",
      "Saving to: ‘romeo.txt’\n",
      "\n",
      "romeo.txt           100%[===================>] 174.79K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2016-09-15 10:50:09 (2.61 MB/s) - ‘romeo.txt’ saved [178983/178983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     163\n",
      "     117\n"
     ]
    }
   ],
   "source": [
    "# The number of Romeo's speaking lines\n",
    "!grep -oE '\\w{{3,}}' romeo.txt | grep -w \"Rom\" | wc -l \n",
    "\n",
    "# The number of Juliet's speaking lines\n",
    "!grep -oE '\\w{{3,}}' romeo.txt | grep -w \"Jul\" | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Capital Bikeshare (40 points)\n",
    "#### Use the data available at https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-andschedule/master/projects/project-01/2016q1.csv.zip for this problem.\n",
    "#### Part A (20 points)\n",
    "#### Which 10 Capital Bikeshare stations were the most popular departing stations in Q1 2016? Which 10 were the most popular destination stations in Q1 2016?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-15 14:40:38--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10643003 (10M) [application/octet-stream]\n",
      "Saving to: ‘2016q1.csv.zip’\n",
      "\n",
      "2016q1.csv.zip      100%[===================>]  10.15M  3.52MB/s    in 2.9s    \n",
      "\n",
      "2016-09-15 14:40:41 (3.52 MB/s) - ‘2016q1.csv.zip’ saved [10643003/10643003]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2016q1.csv.zip\n",
      "  inflating: 2016q1.csv              \n"
     ]
    }
   ],
   "source": [
    "!unzip 2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  the most popular departing stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13120 Columbus Circle / Union Station\r\n",
      "9560 Massachusetts Ave & Dupont Circle NW\r\n",
      "9388 Lincoln Memorial\r\n",
      "8138 Jefferson Dr & 14th St SW\r\n",
      "7479 Thomas Circle\r\n",
      "7401 15th & P St NW\r\n",
      "6568 14th & V St NW\r\n",
      "6491 New Hampshire Ave & T St NW\r\n",
      "5649 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      "5514 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5 2016q1.csv | csvsort -c1 | uniq -c | sort -rn | head -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the most popular destination stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13880 Columbus Circle / Union Station\r\n",
      "11183 Massachusetts Ave & Dupont Circle NW\r\n",
      "9419 Lincoln Memorial\r\n",
      "8975 Jefferson Dr & 14th St SW\r\n",
      "8092 15th & P St NW\r\n",
      "7267 14th & V St NW\r\n",
      "6997 Thomas Circle\r\n",
      "6245 New Hampshire Ave & T St NW\r\n",
      "5761 5th & K St NW\r\n",
      "5651 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7 2016q1.csv | csvsort -c1 | uniq -c | sort -rn | head -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part B (20 points)\n",
    "#### For the most popular departure station, which 10 bikes were used most in trips departing from there? Which 10 bikes were used most in trips ending at the most popular destination station?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10 bikes were used most in trips departing the most popular departure station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W22227\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n",
      "W21867\r\n",
      "W21641\r\n",
      "W21538\r\n",
      "W21239\r\n",
      "W20540\r\n",
      "W00714\r\n",
      "W22080\r\n",
      "W21450\r\n",
      "W21076\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5,8 2016q1.csv | csvgrep -c1 -m 'Columbus Circle / Union Station' \\\n",
    "    | csvsort -c2 | uniq -c | sort -rn | head -10 | csvcut -c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10 bikes were used most in trips departing the most popular destination station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W00485\r\n",
      "sort: write failed: standard output: Broken pipeW22227\r\n",
      "\r\n",
      "sort: write error\r\n",
      "W22099\r\n",
      "W22080\r\n",
      "W21239\r\n",
      "W21076\r\n",
      "W20425\r\n",
      "W00714\r\n",
      "W21997\r\n",
      "W21867\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7,8 2016q1.csv | csvgrep -c1 -m 'Columbus Circle / Union Station' \\\n",
    "    | csvsort -c2 | uniq -c | sort -rn | head -10 | csvcut -c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Filters (20 points)\n",
    "#### In class lectures, previous exercises, and Problems 1 and 2 above, you use Unix commands like grep and tr as filters, changing the text lines streaming through the pipeline. In this problem, write small Python programs that act as filters in the same way, where each program serves one filtering purpose. Name each new filter program clearly.\n",
    "#### For this problem, use the basic Python filter template shown in class and available at https://raw.githubusercontent.com/gwsbistm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/simplefilter.py as the basis of yourown filters.\n",
    "#### Part A (10 points)\n",
    "#### Demonstrate a pipeline that performs a count of the top ten unique words in Little Women. This may be exactly the same pipeline we have used before. \n",
    "#### Write a Python filter than replaces grep -oE '\\w{2,}' to split lines of text into one word per line, and write an additional Python filter to replace tr '[:upper:]' '[:lower:]' to transform text into lower case. \n",
    "#### With your two new filters, repeat the original pipeline, and substitute your new filters as appropriate. You should obtain the same results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-15 15:20:42--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/simplefilter.py\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 211 [text/plain]\n",
      "Saving to: ‘simplefilter.py’\n",
      "\n",
      "simplefilter.py     100%[===================>]     211  --.-KB/s    in 0s      \n",
      "\n",
      "2016-09-15 15:20:43 (28.7 MB/s) - ‘simplefilter.py’ saved [211/211]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/simplefilter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. a Python filter than replaces grep -oE '\\w{2,}' to split lines of text into one word per line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\r\n",
      "Project\r\n",
      "Gutenberg\r\n",
      "EBook\r\n",
      "of\r\n",
      "Little\r\n",
      "Women\r\n",
      "by\r\n",
      "Louisa\r\n",
      "May\r\n",
      "Alcott\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{2,}}' women.txt | head -11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a filter to split the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x filter_split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\r\n",
      "Project\r\n",
      "Gutenberg\r\n",
      "EBook\r\n",
      "of\r\n",
      "Little\r\n",
      "Women\r\n",
      "by\r\n",
      "Louisa\r\n",
      "May\r\n",
      "Alcott\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 women.txt | ./filter_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. a Python filter to replace tr '[:upper:]' '[:lower:]' to transform text into lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿the project gutenberg ebook of little women, by louisa may alcott\r",
      "\r\n",
      "\r",
      "\r\n",
      "this ebook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the project gutenberg license included\r",
      "\r\n",
      "with this ebook or online at www.gutenberg.net\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "title: little women\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head women.txt | tr '[:upper:]' '[:lower:]' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a Python filter to transper  words to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x filter_lower.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿the project gutenberg ebook of little women, by louisa may alcott\r",
      "\r\n",
      "\r",
      "\r\n",
      "this ebook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the project gutenberg license included\r",
      "\r\n",
      "with this ebook or online at www.gutenberg.net\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "title: little women\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head women.txt | ./filter_lower.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With two new filters, repeat the original pipeline, and substitute new filters as appropriate. \n",
    "##### Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8155 and\r\n",
      "7689 the\r\n",
      "5152 to\r\n",
      "3523 of\r\n",
      "3245 her\r\n",
      "2774 it\r\n",
      "2503 in\r\n",
      "2447 you\r\n",
      "2343 she\r\n",
      "2233 for\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{2,}}' women.txt | tr '[:upper:]' '[:lower:]' | sort |uniq -c| sort -rn | head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With two new filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x filter_split.py\n",
    "!chmod +x filter_lower.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8155 and\n",
      "7689 the\n",
      "5152 to\n",
      "3523 of\n",
      "3245 her\n",
      "2774 it\n",
      "2503 in\n",
      "2447 you\n",
      "2343 she\n",
      "2233 for\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!./filter_lower.py women.txt | ./filter_split.py | sort | uniq -c | sort -rn | head -10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part B (10 points)\n",
    "#### Write a Python filter that removes at least ten common words of English text, commonly known as “stop words”. Sources of English stop word lists are readily available online, or you may generate your own list from the text.\n",
    "#### Add your stop word filter to a word count pipeline and show the top 25 words in Little Women with stop words removed. You may re-use your filters from Part A if you wish, although this is not required for full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 25 words in Little Women before removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8155 and\r\n",
      "7689 the\r\n",
      "5152 to\r\n",
      "3523 of\r\n",
      "3245 her\r\n",
      "2774 it\r\n",
      "2503 in\r\n",
      "2447 you\r\n",
      "2343 she\r\n",
      "2233 for\r\n",
      "2033 was\r\n",
      "1978 as\r\n",
      "1937 that\r\n",
      "1854 with\r\n",
      "1598 he\r\n",
      "1469 but\r\n",
      "1362 jo\r\n",
      "1135 so\r\n",
      "1118 his\r\n",
      "1067 at\r\n",
      "1063 had\r\n",
      "1014 be\r\n",
      " 976 on\r\n",
      " 942 not\r\n",
      " 916 if\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{2,}}' women.txt | tr '[:upper:]' '[:lower:]' |  sort | uniq -c |sort -rn |head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The top 25 words in Little Women with stop words removed\n",
    "##### stopwords = ['and','the','to','of','her','in','it','for','you','was','she','as','with','that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x filter_stopwords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598 he\r\n",
      "1469 but\r\n",
      "1362 jo\r\n",
      "1135 so\r\n",
      "1118 his\r\n",
      "1067 at\r\n",
      "1063 had\r\n",
      "1014 be\r\n",
      " 976 on\r\n",
      " 942 not\r\n",
      " 916 if\r\n",
      " 881 all\r\n",
      " 843 my\r\n",
      " 827 said\r\n",
      " 821 is\r\n",
      " 782 him\r\n",
      " 755 me\r\n",
      " 730 little\r\n",
      " 725 one\r\n",
      " 719 they\r\n",
      " 717 have\r\n",
      " 709 when\r\n",
      " 708 do\r\n",
      " 686 meg\r\n",
      " 658 up\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!grep -oE '\\w{{2,}}' women.txt | ./filter_lower.py | ./filter_stopwords.py | sort |uniq -c |sort -rn | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit (10 points)\n",
    "#### Use GNU parallel to count the 25 most common words across all the 109 texts in the zip file provided, with stop words removed. You may re-use your filters from Problem 3.\n",
    "#### Use the texts available at https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-andschedule/master/projects/project-01/texts.zip for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-16 10:28:55--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12668137 (12M) [application/octet-stream]\n",
      "Saving to: ‘texts.zip.1’\n",
      "\n",
      "texts.zip.1         100%[===================>]  12.08M  3.54MB/s    in 3.4s    \n",
      "\n",
      "2016-09-16 10:28:59 (3.52 MB/s) - ‘texts.zip.1’ saved [12668137/12668137]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  texts.zip\n",
      "  inflating: texts/10001.txt         \n",
      "  inflating: texts/10002.txt         \n",
      "  inflating: texts/10003.txt         \n",
      "  inflating: texts/10004.txt         \n",
      "  inflating: texts/10005.txt         \n",
      "  inflating: texts/10006.txt         \n",
      "  inflating: texts/10007.txt         \n",
      "  inflating: texts/10008.txt         \n",
      "  inflating: texts/10009.txt         \n",
      "  inflating: texts/10010.txt         \n",
      "  inflating: texts/10011.txt         \n",
      "  inflating: texts/10012.txt         \n",
      "  inflating: texts/10013.txt         \n",
      "  inflating: texts/10014.txt         \n",
      "  inflating: texts/10015.txt         \n",
      "  inflating: texts/10016.txt         \n",
      "  inflating: texts/10017.txt         \n",
      "  inflating: texts/10018.txt         \n",
      "  inflating: texts/10019.txt         \n",
      "  inflating: texts/10020.txt         \n",
      "  inflating: texts/10021.txt         \n",
      "  inflating: texts/10023.txt         \n",
      "  inflating: texts/10024.txt         \n",
      "  inflating: texts/10025.txt         \n",
      "  inflating: texts/10026.txt         \n",
      "  inflating: texts/10027.txt         \n",
      "  inflating: texts/10028.txt         \n",
      "  inflating: texts/10029.txt         \n",
      "  inflating: texts/10030.txt         \n",
      "  inflating: texts/10031.txt         \n",
      "  inflating: texts/10032.txt         \n",
      "  inflating: texts/10033.txt         \n",
      "  inflating: texts/10034.txt         \n",
      "  inflating: texts/10035.txt         \n",
      "  inflating: texts/10036.txt         \n",
      "  inflating: texts/10037.txt         \n",
      "  inflating: texts/10038.txt         \n",
      "  inflating: texts/10039.txt         \n",
      "  inflating: texts/10040.txt         \n",
      "  inflating: texts/10041.txt         \n",
      "  inflating: texts/10042.txt         \n",
      "  inflating: texts/10043.txt         \n",
      "  inflating: texts/10045.txt         \n",
      "  inflating: texts/10046.txt         \n",
      "  inflating: texts/10047.txt         \n",
      "  inflating: texts/10048.txt         \n",
      "  inflating: texts/10049.txt         \n",
      "  inflating: texts/10050.txt         \n",
      "  inflating: texts/10051.txt         \n",
      "  inflating: texts/10052.txt         \n",
      "  inflating: texts/10056.txt         \n",
      "  inflating: texts/10059.txt         \n",
      "  inflating: texts/10060.txt         \n",
      "  inflating: texts/10062.txt         \n",
      "  inflating: texts/12370.txt         \n",
      "  inflating: texts/12372.txt         \n",
      "  inflating: texts/12373.txt         \n",
      "  inflating: texts/12374.txt         \n",
      "  inflating: texts/12375.txt         \n",
      "  inflating: texts/12376.txt         \n",
      "  inflating: texts/12377.txt         \n",
      "  inflating: texts/12378.txt         \n",
      "  inflating: texts/12380.txt         \n",
      "  inflating: texts/12381.txt         \n",
      "  inflating: texts/12383.txt         \n",
      "  inflating: texts/12384.txt         \n",
      "  inflating: texts/12385.txt         \n",
      "  inflating: texts/12386.txt         \n",
      "  inflating: texts/1jcfs10.txt       \n",
      "  inflating: texts/2babb10.txt       \n",
      "  inflating: texts/3babb10.txt       \n",
      "  inflating: texts/50bab10.txt       \n",
      "  inflating: texts/ajtl10.txt        \n",
      "  inflating: texts/allyr10.txt       \n",
      "  inflating: texts/alpsn10.txt       \n",
      "  inflating: texts/balen10.txt       \n",
      "  inflating: texts/baleng2.txt       \n",
      "  inflating: texts/batlf10.txt       \n",
      "  inflating: texts/bgopr10.txt       \n",
      "  inflating: texts/brnte10.txt       \n",
      "  inflating: texts/bstjg10.txt       \n",
      "  inflating: texts/cambp10.txt       \n",
      "  inflating: texts/canbe10.txt       \n",
      "  inflating: texts/cantp10.txt       \n",
      "  inflating: texts/cfrz10.txt        \n",
      "  inflating: texts/crsnk10.txt       \n",
      "  inflating: texts/esbio10.txt       \n",
      "  inflating: texts/grybr10.txt       \n",
      "  inflating: texts/mklmt10.txt       \n",
      "  inflating: texts/morem10.txt       \n",
      "  inflating: texts/mspcd10.txt       \n",
      "  inflating: texts/penbr10.txt       \n",
      "  inflating: texts/pgjr10.txt        \n",
      "  inflating: texts/pntvw10.txt       \n",
      "  inflating: texts/prcpg10.txt       \n",
      "  inflating: texts/prhg10.txt        \n",
      "  inflating: texts/prhsb10.txt       \n",
      "  inflating: texts/rlsl110.txt       \n",
      "  inflating: texts/rlsl210.txt       \n",
      "  inflating: texts/rmlav10.txt       \n",
      "  inflating: texts/sesli10.txt       \n",
      "  inflating: texts/svyrd10.txt       \n",
      "  inflating: texts/tecom10.txt       \n",
      "  inflating: texts/utrkj10.txt       \n",
      "  inflating: texts/vpasm10.txt       \n",
      "  inflating: texts/wldsp10.txt       \n",
      "  inflating: texts/wtrbs10.txt       \n",
      "  inflating: texts/zncli10.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip -d texts texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x filter_stopwords.py\n",
    "!chmod +x filter_lower.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic tradition requires you to cite works you base your article on.\n",
      "When using programs that use GNU Parallel to process data for publication\n",
      "please cite:\n",
      "\n",
      "  O. Tange (2011): GNU Parallel - The Command-Line Power Tool,\n",
      "  ;login: The USENIX Magazine, February 2011:42-47.\n",
      "\n",
      "This helps funding further development; AND IT WON'T COST YOU A CENT.\n",
      "If you pay 10000 EUR you should feel free to use GNU Parallel without citing.\n",
      "\n",
      "To silence the citation notice: run 'parallel --citation'.\n",
      "\n",
      "\n",
      "Computers / CPU cores / Max jobs to run\n",
      "1:local / 4 / 4\n",
      "\n",
      "Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete\n",
      "ETA: 0s Left: 0 AVG: 0.13s  local:0/108/100%/0.1s \n",
      "53182 he\n",
      "47931 is\n",
      "42855 his\n",
      "33086 on\n",
      "33034 but\n",
      "32072 not\n",
      "31664 at\n",
      "31320 had\n",
      "30546 be\n",
      "28333 by\n",
      "27847 this\n",
      "26361 my\n",
      "25686 or\n",
      "25282 have\n",
      "25142 all\n",
      "23930 which\n",
      "23772 they\n",
      "23738 from\n",
      "21893 we\n",
      "20666 are\n",
      "19930 me\n",
      "19591 one\n",
      "19123 him\n",
      "18644 were\n",
      "18273 so\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "\n",
      "real\t0m15.749s\n",
      "user\t0m43.188s\n",
      "sys\t0m5.123s\n"
     ]
    }
   ],
   "source": [
    "!time ls texts/*.txt \\\n",
    "    | parallel --eta -j+0 \"grep -oE '\\w{2,}' {} | ./filter_lower.py |./filter_stopwords.py \" \\\n",
    "    | sort | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
